{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.286160\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.138109 analytic: 1.138109, relative error: 5.659742e-08\n",
      "numerical: -3.218544 analytic: -3.218544, relative error: 1.782991e-08\n",
      "numerical: 1.691509 analytic: 1.691509, relative error: 1.059737e-08\n",
      "numerical: -3.375061 analytic: -3.375061, relative error: 6.945477e-09\n",
      "numerical: -1.736176 analytic: -1.736176, relative error: 4.871533e-08\n",
      "numerical: -1.297688 analytic: -1.297688, relative error: 1.987391e-08\n",
      "numerical: 0.360056 analytic: 0.360056, relative error: 4.523833e-08\n",
      "numerical: -2.508038 analytic: -2.508038, relative error: 2.514132e-08\n",
      "numerical: 0.370552 analytic: 0.370552, relative error: 5.235906e-08\n",
      "numerical: -1.232206 analytic: -1.232206, relative error: 2.185797e-08\n",
      "numerical: -1.993985 analytic: -1.993985, relative error: 7.524525e-09\n",
      "numerical: -2.364627 analytic: -2.364627, relative error: 1.996564e-08\n",
      "numerical: -1.379800 analytic: -1.379800, relative error: 7.517953e-08\n",
      "numerical: -1.688418 analytic: -1.688418, relative error: 5.251078e-09\n",
      "numerical: -1.058218 analytic: -1.058218, relative error: 2.164581e-08\n",
      "numerical: 1.134595 analytic: 1.134595, relative error: 5.968983e-09\n",
      "numerical: -1.637446 analytic: -1.637446, relative error: 5.384765e-09\n",
      "numerical: -0.829736 analytic: -0.829736, relative error: 3.711445e-08\n",
      "numerical: -3.640981 analytic: -3.640981, relative error: 7.297258e-09\n",
      "numerical: -1.927682 analytic: -1.927682, relative error: 1.217429e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.286160e+00 computed in 0.043310s\n",
      "vectorized loss: 2.286160e+00 computed in 0.004183s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 14.087161\n",
      "iteration 100 / 1500: loss 12.372379\n",
      "iteration 200 / 1500: loss 11.710628\n",
      "iteration 300 / 1500: loss 11.799659\n",
      "iteration 400 / 1500: loss 11.048481\n",
      "iteration 500 / 1500: loss 10.972248\n",
      "iteration 600 / 1500: loss 10.559743\n",
      "iteration 700 / 1500: loss 10.659601\n",
      "iteration 800 / 1500: loss 10.265493\n",
      "iteration 900 / 1500: loss 10.108228\n",
      "iteration 1000 / 1500: loss 10.003599\n",
      "iteration 1100 / 1500: loss 9.850938\n",
      "iteration 1200 / 1500: loss 9.652920\n",
      "iteration 1300 / 1500: loss 9.694407\n",
      "iteration 1400 / 1500: loss 9.221671\n",
      "iteration 0 / 1500: loss 10.808300\n",
      "iteration 100 / 1500: loss 8.871245\n",
      "iteration 200 / 1500: loss 8.170617\n",
      "iteration 300 / 1500: loss 7.858514\n",
      "iteration 400 / 1500: loss 7.596320\n",
      "iteration 500 / 1500: loss 7.412405\n",
      "iteration 600 / 1500: loss 7.043705\n",
      "iteration 700 / 1500: loss 7.304590\n",
      "iteration 800 / 1500: loss 7.198732\n",
      "iteration 900 / 1500: loss 6.979493\n",
      "iteration 1000 / 1500: loss 6.758621\n",
      "iteration 1100 / 1500: loss 6.688478\n",
      "iteration 1200 / 1500: loss 6.883526\n",
      "iteration 1300 / 1500: loss 6.644653\n",
      "iteration 1400 / 1500: loss 6.666346\n",
      "iteration 0 / 1500: loss 99.598126\n",
      "iteration 100 / 1500: loss 82.872400\n",
      "iteration 200 / 1500: loss 70.413625\n",
      "iteration 300 / 1500: loss 59.865569\n",
      "iteration 400 / 1500: loss 50.847313\n",
      "iteration 500 / 1500: loss 43.151903\n",
      "iteration 600 / 1500: loss 36.678533\n",
      "iteration 700 / 1500: loss 31.275722\n",
      "iteration 800 / 1500: loss 26.786038\n",
      "iteration 900 / 1500: loss 22.898889\n",
      "iteration 1000 / 1500: loss 19.818983\n",
      "iteration 1100 / 1500: loss 17.084511\n",
      "iteration 1200 / 1500: loss 14.664991\n",
      "iteration 1300 / 1500: loss 12.645837\n",
      "iteration 1400 / 1500: loss 10.966904\n",
      "iteration 0 / 1500: loss 9002.597209\n",
      "iteration 100 / 1500: loss 2.266524\n",
      "iteration 200 / 1500: loss 2.247608\n",
      "iteration 300 / 1500: loss 2.241810\n",
      "iteration 400 / 1500: loss 2.232173\n",
      "iteration 500 / 1500: loss 2.258705\n",
      "iteration 600 / 1500: loss 2.257638\n",
      "iteration 700 / 1500: loss 2.252933\n",
      "iteration 800 / 1500: loss 2.253947\n",
      "iteration 900 / 1500: loss 2.249643\n",
      "iteration 1000 / 1500: loss 2.229766\n",
      "iteration 1100 / 1500: loss 2.250452\n",
      "iteration 1200 / 1500: loss 2.224777\n",
      "iteration 1300 / 1500: loss 2.246968\n",
      "iteration 1400 / 1500: loss 2.256974\n",
      "iteration 0 / 1500: loss 1515.090638\n",
      "iteration 100 / 1500: loss 113.714531\n",
      "iteration 200 / 1500: loss 10.373078\n",
      "iteration 300 / 1500: loss 2.794182\n",
      "iteration 400 / 1500: loss 2.197693\n",
      "iteration 500 / 1500: loss 2.147353\n",
      "iteration 600 / 1500: loss 2.118571\n",
      "iteration 700 / 1500: loss 2.172800\n",
      "iteration 800 / 1500: loss 2.150172\n",
      "iteration 900 / 1500: loss 2.129092\n",
      "iteration 1000 / 1500: loss 2.165255\n",
      "iteration 1100 / 1500: loss 2.152734\n",
      "iteration 1200 / 1500: loss 2.137566\n",
      "iteration 1300 / 1500: loss 2.143917\n",
      "iteration 1400 / 1500: loss 2.170953\n",
      "iteration 0 / 1500: loss 13023.453873\n",
      "iteration 100 / 1500: loss 2.268461\n",
      "iteration 200 / 1500: loss 2.263134\n",
      "iteration 300 / 1500: loss 2.272013\n",
      "iteration 400 / 1500: loss 2.274974\n",
      "iteration 500 / 1500: loss 2.274566\n",
      "iteration 600 / 1500: loss 2.259793\n",
      "iteration 700 / 1500: loss 2.269595\n",
      "iteration 800 / 1500: loss 2.264136\n",
      "iteration 900 / 1500: loss 2.277340\n",
      "iteration 1000 / 1500: loss 2.261795\n",
      "iteration 1100 / 1500: loss 2.269430\n",
      "iteration 1200 / 1500: loss 2.274833\n",
      "iteration 1300 / 1500: loss 2.269581\n",
      "iteration 1400 / 1500: loss 2.254909\n",
      "iteration 0 / 1500: loss 830.738885\n",
      "iteration 100 / 1500: loss 203.342293\n",
      "iteration 200 / 1500: loss 51.088931\n",
      "iteration 300 / 1500: loss 13.920015\n",
      "iteration 400 / 1500: loss 4.989345\n",
      "iteration 500 / 1500: loss 2.716061\n",
      "iteration 600 / 1500: loss 2.298275\n",
      "iteration 700 / 1500: loss 2.135249\n",
      "iteration 800 / 1500: loss 2.136123\n",
      "iteration 900 / 1500: loss 2.153077\n",
      "iteration 1000 / 1500: loss 2.094496\n",
      "iteration 1100 / 1500: loss 2.110378\n",
      "iteration 1200 / 1500: loss 2.059616\n",
      "iteration 1300 / 1500: loss 2.117823\n",
      "iteration 1400 / 1500: loss 2.073819\n",
      "iteration 0 / 1500: loss 483.482615\n",
      "iteration 100 / 1500: loss 209.742777\n",
      "iteration 200 / 1500: loss 91.941477\n",
      "iteration 300 / 1500: loss 41.015550\n",
      "iteration 400 / 1500: loss 18.899641\n",
      "iteration 500 / 1500: loss 9.369730\n",
      "iteration 600 / 1500: loss 5.127154\n",
      "iteration 700 / 1500: loss 3.381472\n",
      "iteration 800 / 1500: loss 2.594479\n",
      "iteration 900 / 1500: loss 2.294435\n",
      "iteration 1000 / 1500: loss 2.219126\n",
      "iteration 1100 / 1500: loss 2.109081\n",
      "iteration 1200 / 1500: loss 2.010608\n",
      "iteration 1300 / 1500: loss 2.043691\n",
      "iteration 1400 / 1500: loss 2.051487\n",
      "iteration 0 / 1500: loss 131.053557\n",
      "iteration 100 / 1500: loss 103.760500\n",
      "iteration 200 / 1500: loss 83.852334\n",
      "iteration 300 / 1500: loss 68.196460\n",
      "iteration 400 / 1500: loss 55.290121\n",
      "iteration 500 / 1500: loss 44.980806\n",
      "iteration 600 / 1500: loss 36.604001\n",
      "iteration 700 / 1500: loss 29.963198\n",
      "iteration 800 / 1500: loss 24.689376\n",
      "iteration 900 / 1500: loss 20.286377\n",
      "iteration 1000 / 1500: loss 16.624078\n",
      "iteration 1100 / 1500: loss 13.901838\n",
      "iteration 1200 / 1500: loss 11.673487\n",
      "iteration 1300 / 1500: loss 9.682466\n",
      "iteration 1400 / 1500: loss 8.303982\n",
      "iteration 0 / 1500: loss 118.925309\n",
      "iteration 100 / 1500: loss 96.332990\n",
      "iteration 200 / 1500: loss 79.371429\n",
      "iteration 300 / 1500: loss 65.103957\n",
      "iteration 400 / 1500: loss 53.718037\n",
      "iteration 500 / 1500: loss 44.357812\n",
      "iteration 600 / 1500: loss 36.677854\n",
      "iteration 700 / 1500: loss 30.443013\n",
      "iteration 800 / 1500: loss 25.369663\n",
      "iteration 900 / 1500: loss 21.117729\n",
      "iteration 1000 / 1500: loss 17.666013\n",
      "iteration 1100 / 1500: loss 14.908438\n",
      "iteration 1200 / 1500: loss 12.564741\n",
      "iteration 1300 / 1500: loss 10.642207\n",
      "iteration 1400 / 1500: loss 9.052355\n",
      "iteration 0 / 1500: loss 14.203019\n",
      "iteration 100 / 1500: loss 7.623547\n",
      "iteration 200 / 1500: loss 5.494023\n",
      "iteration 300 / 1500: loss 4.117228\n",
      "iteration 400 / 1500: loss 3.326045\n",
      "iteration 500 / 1500: loss 2.781166\n",
      "iteration 600 / 1500: loss 2.358391\n",
      "iteration 700 / 1500: loss 2.250260\n",
      "iteration 800 / 1500: loss 2.155697\n",
      "iteration 900 / 1500: loss 1.899549\n",
      "iteration 1000 / 1500: loss 1.931553\n",
      "iteration 1100 / 1500: loss 1.813698\n",
      "iteration 1200 / 1500: loss 1.887890\n",
      "iteration 1300 / 1500: loss 1.837298\n",
      "iteration 1400 / 1500: loss 1.838849\n",
      "iteration 0 / 1500: loss 10.910823\n",
      "iteration 100 / 1500: loss 5.765698\n",
      "iteration 200 / 1500: loss 4.927185\n",
      "iteration 300 / 1500: loss 4.300425\n",
      "iteration 400 / 1500: loss 3.704049\n",
      "iteration 500 / 1500: loss 3.201282\n",
      "iteration 600 / 1500: loss 2.833014\n",
      "iteration 700 / 1500: loss 2.604249\n",
      "iteration 800 / 1500: loss 2.554549\n",
      "iteration 900 / 1500: loss 2.278436\n",
      "iteration 1000 / 1500: loss 2.222632\n",
      "iteration 1100 / 1500: loss 2.152522\n",
      "iteration 1200 / 1500: loss 1.972643\n",
      "iteration 1300 / 1500: loss 2.058368\n",
      "iteration 1400 / 1500: loss 1.969450\n",
      "iteration 0 / 1500: loss 101.302757\n",
      "iteration 100 / 1500: loss 2.819809\n",
      "iteration 200 / 1500: loss 1.955328\n",
      "iteration 300 / 1500: loss 1.957811\n",
      "iteration 400 / 1500: loss 1.876919\n",
      "iteration 500 / 1500: loss 1.961473\n",
      "iteration 600 / 1500: loss 1.931837\n",
      "iteration 700 / 1500: loss 2.031243\n",
      "iteration 800 / 1500: loss 1.948821\n",
      "iteration 900 / 1500: loss 2.053569\n",
      "iteration 1000 / 1500: loss 1.922374\n",
      "iteration 1100 / 1500: loss 1.997515\n",
      "iteration 1200 / 1500: loss 2.014483\n",
      "iteration 1300 / 1500: loss 2.071637\n",
      "iteration 1400 / 1500: loss 1.893802\n",
      "iteration 0 / 1500: loss 8837.214320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:91: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(-np.log(score[range(num_samp),y] / f))\n",
      "cs231n/classifiers/softmax.py:89: RuntimeWarning: overflow encountered in exp\n",
      "  score = np.exp(np.dot(X,W))\n",
      "cs231n/classifiers/softmax.py:94: RuntimeWarning: invalid value encountered in divide\n",
      "  p = score / np.reshape(f,[-1,1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1517.306235\n",
      "iteration 100 / 1500: loss 2.295382\n",
      "iteration 200 / 1500: loss 2.235586\n",
      "iteration 300 / 1500: loss 2.415794\n",
      "iteration 400 / 1500: loss 2.417532\n",
      "iteration 500 / 1500: loss 2.234929\n",
      "iteration 600 / 1500: loss 2.308794\n",
      "iteration 700 / 1500: loss 2.243359\n",
      "iteration 800 / 1500: loss 2.238727\n",
      "iteration 900 / 1500: loss 2.260181\n",
      "iteration 1000 / 1500: loss 2.334625\n",
      "iteration 1100 / 1500: loss 2.358883\n",
      "iteration 1200 / 1500: loss 2.190755\n",
      "iteration 1300 / 1500: loss 2.449619\n",
      "iteration 1400 / 1500: loss 2.332005\n",
      "iteration 0 / 1500: loss 13175.163228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:91: RuntimeWarning: invalid value encountered in divide\n",
      "  loss = np.sum(-np.log(score[range(num_samp),y] / f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 828.915973\n",
      "iteration 100 / 1500: loss 2.197375\n",
      "iteration 200 / 1500: loss 2.268246\n",
      "iteration 300 / 1500: loss 2.173820\n",
      "iteration 400 / 1500: loss 2.203191\n",
      "iteration 500 / 1500: loss 2.167902\n",
      "iteration 600 / 1500: loss 2.207461\n",
      "iteration 700 / 1500: loss 2.235124\n",
      "iteration 800 / 1500: loss 2.216528\n",
      "iteration 900 / 1500: loss 2.113033\n",
      "iteration 1000 / 1500: loss 2.271608\n",
      "iteration 1100 / 1500: loss 2.161971\n",
      "iteration 1200 / 1500: loss 2.114814\n",
      "iteration 1300 / 1500: loss 2.160546\n",
      "iteration 1400 / 1500: loss 2.202585\n",
      "iteration 0 / 1500: loss 492.957957\n",
      "iteration 100 / 1500: loss 2.180933\n",
      "iteration 200 / 1500: loss 2.162703\n",
      "iteration 300 / 1500: loss 2.190829\n",
      "iteration 400 / 1500: loss 2.165799\n",
      "iteration 500 / 1500: loss 2.144286\n",
      "iteration 600 / 1500: loss 2.098933\n",
      "iteration 700 / 1500: loss 2.103987\n",
      "iteration 800 / 1500: loss 2.080275\n",
      "iteration 900 / 1500: loss 2.170729\n",
      "iteration 1000 / 1500: loss 2.061185\n",
      "iteration 1100 / 1500: loss 2.092331\n",
      "iteration 1200 / 1500: loss 2.107252\n",
      "iteration 1300 / 1500: loss 2.130595\n",
      "iteration 1400 / 1500: loss 2.175209\n",
      "iteration 0 / 1500: loss 127.675437\n",
      "iteration 100 / 1500: loss 2.396946\n",
      "iteration 200 / 1500: loss 2.136578\n",
      "iteration 300 / 1500: loss 2.076007\n",
      "iteration 400 / 1500: loss 1.848717\n",
      "iteration 500 / 1500: loss 2.020352\n",
      "iteration 600 / 1500: loss 1.997893\n",
      "iteration 700 / 1500: loss 2.032428\n",
      "iteration 800 / 1500: loss 2.042504\n",
      "iteration 900 / 1500: loss 2.083434\n",
      "iteration 1000 / 1500: loss 1.939334\n",
      "iteration 1100 / 1500: loss 1.936351\n",
      "iteration 1200 / 1500: loss 1.965489\n",
      "iteration 1300 / 1500: loss 2.031662\n",
      "iteration 1400 / 1500: loss 1.920781\n",
      "iteration 0 / 1500: loss 122.130561\n",
      "iteration 100 / 1500: loss 2.470070\n",
      "iteration 200 / 1500: loss 1.972672\n",
      "iteration 300 / 1500: loss 1.957196\n",
      "iteration 400 / 1500: loss 1.895404\n",
      "iteration 500 / 1500: loss 2.005046\n",
      "iteration 600 / 1500: loss 1.967504\n",
      "iteration 700 / 1500: loss 1.873101\n",
      "iteration 800 / 1500: loss 1.894699\n",
      "iteration 900 / 1500: loss 1.986835\n",
      "iteration 1000 / 1500: loss 2.003863\n",
      "iteration 1100 / 1500: loss 1.930091\n",
      "iteration 1200 / 1500: loss 1.932688\n",
      "iteration 1300 / 1500: loss 1.931528\n",
      "iteration 1400 / 1500: loss 1.864196\n",
      "iteration 0 / 1500: loss 15.143453\n",
      "iteration 100 / 1500: loss 13.885377\n",
      "iteration 200 / 1500: loss 12.513105\n",
      "iteration 300 / 1500: loss 12.203884\n",
      "iteration 400 / 1500: loss 11.804891\n",
      "iteration 500 / 1500: loss 11.830347\n",
      "iteration 600 / 1500: loss 11.618301\n",
      "iteration 700 / 1500: loss 11.233808\n",
      "iteration 800 / 1500: loss 11.240445\n",
      "iteration 900 / 1500: loss 11.139556\n",
      "iteration 1000 / 1500: loss 11.044156\n",
      "iteration 1100 / 1500: loss 11.079638\n",
      "iteration 1200 / 1500: loss 10.798804\n",
      "iteration 1300 / 1500: loss 10.898085\n",
      "iteration 1400 / 1500: loss 10.648029\n",
      "iteration 0 / 1500: loss 10.470426\n",
      "iteration 100 / 1500: loss 9.328579\n",
      "iteration 200 / 1500: loss 8.461035\n",
      "iteration 300 / 1500: loss 8.097527\n",
      "iteration 400 / 1500: loss 8.250515\n",
      "iteration 500 / 1500: loss 8.015116\n",
      "iteration 600 / 1500: loss 8.051481\n",
      "iteration 700 / 1500: loss 7.951377\n",
      "iteration 800 / 1500: loss 7.359242\n",
      "iteration 900 / 1500: loss 7.582564\n",
      "iteration 1000 / 1500: loss 7.237493\n",
      "iteration 1100 / 1500: loss 7.445010\n",
      "iteration 1200 / 1500: loss 7.741679\n",
      "iteration 1300 / 1500: loss 7.371172\n",
      "iteration 1400 / 1500: loss 7.225130\n",
      "iteration 0 / 1500: loss 102.366388\n",
      "iteration 100 / 1500: loss 93.566769\n",
      "iteration 200 / 1500: loss 86.575696\n",
      "iteration 300 / 1500: loss 80.349136\n",
      "iteration 400 / 1500: loss 74.344869\n",
      "iteration 500 / 1500: loss 69.190774\n",
      "iteration 600 / 1500: loss 64.145030\n",
      "iteration 700 / 1500: loss 59.675252\n",
      "iteration 800 / 1500: loss 55.456429\n",
      "iteration 900 / 1500: loss 51.548429\n",
      "iteration 1000 / 1500: loss 48.112194\n",
      "iteration 1100 / 1500: loss 44.729572\n",
      "iteration 1200 / 1500: loss 41.587888\n",
      "iteration 1300 / 1500: loss 38.811851\n",
      "iteration 1400 / 1500: loss 36.098661\n",
      "iteration 0 / 1500: loss 9009.444390\n",
      "iteration 100 / 1500: loss 10.550535\n",
      "iteration 200 / 1500: loss 2.231868\n",
      "iteration 300 / 1500: loss 2.257205\n",
      "iteration 400 / 1500: loss 2.245267\n",
      "iteration 500 / 1500: loss 2.238702\n",
      "iteration 600 / 1500: loss 2.240982\n",
      "iteration 700 / 1500: loss 2.260383\n",
      "iteration 800 / 1500: loss 2.244221\n",
      "iteration 900 / 1500: loss 2.247238\n",
      "iteration 1000 / 1500: loss 2.249931\n",
      "iteration 1100 / 1500: loss 2.231168\n",
      "iteration 1200 / 1500: loss 2.255581\n",
      "iteration 1300 / 1500: loss 2.244732\n",
      "iteration 1400 / 1500: loss 2.240376\n",
      "iteration 0 / 1500: loss 1506.073263\n",
      "iteration 100 / 1500: loss 474.965386\n",
      "iteration 200 / 1500: loss 150.894559\n",
      "iteration 300 / 1500: loss 48.906619\n",
      "iteration 400 / 1500: loss 16.901988\n",
      "iteration 500 / 1500: loss 6.760043\n",
      "iteration 600 / 1500: loss 3.555242\n",
      "iteration 700 / 1500: loss 2.572784\n",
      "iteration 800 / 1500: loss 2.263271\n",
      "iteration 900 / 1500: loss 2.202584\n",
      "iteration 1000 / 1500: loss 2.109027\n",
      "iteration 1100 / 1500: loss 2.129317\n",
      "iteration 1200 / 1500: loss 2.140111\n",
      "iteration 1300 / 1500: loss 2.127879\n",
      "iteration 1400 / 1500: loss 2.135022\n",
      "iteration 0 / 1500: loss 12969.313893\n",
      "iteration 100 / 1500: loss 2.780984\n",
      "iteration 200 / 1500: loss 2.259804\n",
      "iteration 300 / 1500: loss 2.272125\n",
      "iteration 400 / 1500: loss 2.265757\n",
      "iteration 500 / 1500: loss 2.270254\n",
      "iteration 600 / 1500: loss 2.253651\n",
      "iteration 700 / 1500: loss 2.257052\n",
      "iteration 800 / 1500: loss 2.262125\n",
      "iteration 900 / 1500: loss 2.269514\n",
      "iteration 1000 / 1500: loss 2.284991\n",
      "iteration 1100 / 1500: loss 2.248891\n",
      "iteration 1200 / 1500: loss 2.265570\n",
      "iteration 1300 / 1500: loss 2.259039\n",
      "iteration 1400 / 1500: loss 2.285636\n",
      "iteration 0 / 1500: loss 820.700778\n",
      "iteration 100 / 1500: loss 438.848661\n",
      "iteration 200 / 1500: loss 235.017311\n",
      "iteration 300 / 1500: loss 126.223348\n",
      "iteration 400 / 1500: loss 68.354339\n",
      "iteration 500 / 1500: loss 37.465152\n",
      "iteration 600 / 1500: loss 20.947682\n",
      "iteration 700 / 1500: loss 12.118104\n",
      "iteration 800 / 1500: loss 7.501009\n",
      "iteration 900 / 1500: loss 4.933242\n",
      "iteration 1000 / 1500: loss 3.612013\n",
      "iteration 1100 / 1500: loss 2.917672\n",
      "iteration 1200 / 1500: loss 2.509022\n",
      "iteration 1300 / 1500: loss 2.323957\n",
      "iteration 1400 / 1500: loss 2.205975\n",
      "iteration 0 / 1500: loss 493.164083\n",
      "iteration 100 / 1500: loss 340.023190\n",
      "iteration 200 / 1500: loss 234.882269\n",
      "iteration 300 / 1500: loss 162.296734\n",
      "iteration 400 / 1500: loss 112.516170\n",
      "iteration 500 / 1500: loss 78.079402\n",
      "iteration 600 / 1500: loss 54.431733\n",
      "iteration 700 / 1500: loss 38.131363\n",
      "iteration 800 / 1500: loss 26.910792\n",
      "iteration 900 / 1500: loss 19.189485\n",
      "iteration 1000 / 1500: loss 13.872083\n",
      "iteration 1100 / 1500: loss 10.211233\n",
      "iteration 1200 / 1500: loss 7.704293\n",
      "iteration 1300 / 1500: loss 5.906999\n",
      "iteration 1400 / 1500: loss 4.741736\n",
      "iteration 0 / 1500: loss 129.460873\n",
      "iteration 100 / 1500: loss 115.851206\n",
      "iteration 200 / 1500: loss 104.497434\n",
      "iteration 300 / 1500: loss 95.219151\n",
      "iteration 400 / 1500: loss 86.584176\n",
      "iteration 500 / 1500: loss 78.970264\n",
      "iteration 600 / 1500: loss 71.641578\n",
      "iteration 700 / 1500: loss 65.415620\n",
      "iteration 800 / 1500: loss 59.605376\n",
      "iteration 900 / 1500: loss 54.246328\n",
      "iteration 1000 / 1500: loss 49.597978\n",
      "iteration 1100 / 1500: loss 45.400746\n",
      "iteration 1200 / 1500: loss 41.362698\n",
      "iteration 1300 / 1500: loss 37.732055\n",
      "iteration 1400 / 1500: loss 34.488842\n",
      "iteration 0 / 1500: loss 120.488182\n",
      "iteration 100 / 1500: loss 108.543094\n",
      "iteration 200 / 1500: loss 99.129438\n",
      "iteration 300 / 1500: loss 90.781643\n",
      "iteration 400 / 1500: loss 82.806039\n",
      "iteration 500 / 1500: loss 75.892374\n",
      "iteration 600 / 1500: loss 69.663514\n",
      "iteration 700 / 1500: loss 63.943214\n",
      "iteration 800 / 1500: loss 58.806620\n",
      "iteration 900 / 1500: loss 53.917956\n",
      "iteration 1000 / 1500: loss 49.398373\n",
      "iteration 1100 / 1500: loss 45.416772\n",
      "iteration 1200 / 1500: loss 41.715514\n",
      "iteration 1300 / 1500: loss 38.292332\n",
      "iteration 1400 / 1500: loss 35.405330\n",
      "iteration 0 / 1500: loss 14.178225\n",
      "iteration 100 / 1500: loss 11.356441\n",
      "iteration 200 / 1500: loss 10.710865\n",
      "iteration 300 / 1500: loss 10.080286\n",
      "iteration 400 / 1500: loss 9.775943\n",
      "iteration 500 / 1500: loss 9.320411\n",
      "iteration 600 / 1500: loss 8.837058\n",
      "iteration 700 / 1500: loss 8.532165\n",
      "iteration 800 / 1500: loss 8.181207\n",
      "iteration 900 / 1500: loss 7.960397\n",
      "iteration 1000 / 1500: loss 7.446331\n",
      "iteration 1100 / 1500: loss 7.339777\n",
      "iteration 1200 / 1500: loss 6.875434\n",
      "iteration 1300 / 1500: loss 6.637985\n",
      "iteration 1400 / 1500: loss 6.351715\n",
      "iteration 0 / 1500: loss 10.131948\n",
      "iteration 100 / 1500: loss 7.948906\n",
      "iteration 200 / 1500: loss 7.429240\n",
      "iteration 300 / 1500: loss 7.277247\n",
      "iteration 400 / 1500: loss 7.149370\n",
      "iteration 500 / 1500: loss 6.730536\n",
      "iteration 600 / 1500: loss 6.438784\n",
      "iteration 700 / 1500: loss 6.343204\n",
      "iteration 800 / 1500: loss 6.310480\n",
      "iteration 900 / 1500: loss 5.994860\n",
      "iteration 1000 / 1500: loss 5.965455\n",
      "iteration 1100 / 1500: loss 5.721420\n",
      "iteration 1200 / 1500: loss 5.529005\n",
      "iteration 1300 / 1500: loss 5.559746\n",
      "iteration 1400 / 1500: loss 5.189831\n",
      "iteration 0 / 1500: loss 101.257758\n",
      "iteration 100 / 1500: loss 60.807611\n",
      "iteration 200 / 1500: loss 37.819850\n",
      "iteration 300 / 1500: loss 23.839327\n",
      "iteration 400 / 1500: loss 15.343778\n",
      "iteration 500 / 1500: loss 9.982226\n",
      "iteration 600 / 1500: loss 6.812637\n",
      "iteration 700 / 1500: loss 4.954303\n",
      "iteration 800 / 1500: loss 3.774280\n",
      "iteration 900 / 1500: loss 2.972611\n",
      "iteration 1000 / 1500: loss 2.570376\n",
      "iteration 1100 / 1500: loss 2.336896\n",
      "iteration 1200 / 1500: loss 2.188523\n",
      "iteration 1300 / 1500: loss 2.146968\n",
      "iteration 1400 / 1500: loss 2.111881\n",
      "iteration 0 / 1500: loss 8940.170274\n",
      "iteration 100 / 1500: loss 2.264489\n",
      "iteration 200 / 1500: loss 2.255156\n",
      "iteration 300 / 1500: loss 2.269386\n",
      "iteration 400 / 1500: loss 2.240047\n",
      "iteration 500 / 1500: loss 2.249873\n",
      "iteration 600 / 1500: loss 2.265434\n",
      "iteration 700 / 1500: loss 2.236717\n",
      "iteration 800 / 1500: loss 2.261594\n",
      "iteration 900 / 1500: loss 2.253153\n",
      "iteration 1000 / 1500: loss 2.256155\n",
      "iteration 1100 / 1500: loss 2.281188\n",
      "iteration 1200 / 1500: loss 2.251948\n",
      "iteration 1300 / 1500: loss 2.254371\n",
      "iteration 1400 / 1500: loss 2.256663\n",
      "iteration 0 / 1500: loss 1514.669537\n",
      "iteration 100 / 1500: loss 2.762421\n",
      "iteration 200 / 1500: loss 2.100405\n",
      "iteration 300 / 1500: loss 2.134887\n",
      "iteration 400 / 1500: loss 2.140612\n",
      "iteration 500 / 1500: loss 2.128286\n",
      "iteration 600 / 1500: loss 2.161006\n",
      "iteration 700 / 1500: loss 2.135957\n",
      "iteration 800 / 1500: loss 2.150301\n",
      "iteration 900 / 1500: loss 2.133567\n",
      "iteration 1000 / 1500: loss 2.165805\n",
      "iteration 1100 / 1500: loss 2.145640\n",
      "iteration 1200 / 1500: loss 2.140655\n",
      "iteration 1300 / 1500: loss 2.170948\n",
      "iteration 1400 / 1500: loss 2.176897\n",
      "iteration 0 / 1500: loss 12973.167050\n",
      "iteration 100 / 1500: loss 2.266124\n",
      "iteration 200 / 1500: loss 2.258393\n",
      "iteration 300 / 1500: loss 2.262765\n",
      "iteration 400 / 1500: loss 2.275928\n",
      "iteration 500 / 1500: loss 2.271623\n",
      "iteration 600 / 1500: loss 2.256413\n",
      "iteration 700 / 1500: loss 2.252562\n",
      "iteration 800 / 1500: loss 2.275690\n",
      "iteration 900 / 1500: loss 2.240519\n",
      "iteration 1000 / 1500: loss 2.270780\n",
      "iteration 1100 / 1500: loss 2.251727\n",
      "iteration 1200 / 1500: loss 2.284796\n",
      "iteration 1300 / 1500: loss 2.260364\n",
      "iteration 1400 / 1500: loss 2.261348\n",
      "iteration 0 / 1500: loss 821.360331\n",
      "iteration 100 / 1500: loss 14.308220\n",
      "iteration 200 / 1500: loss 2.299371\n",
      "iteration 300 / 1500: loss 2.091403\n",
      "iteration 400 / 1500: loss 2.168430\n",
      "iteration 500 / 1500: loss 2.102225\n",
      "iteration 600 / 1500: loss 2.138405\n",
      "iteration 700 / 1500: loss 2.056609\n",
      "iteration 800 / 1500: loss 2.138460\n",
      "iteration 900 / 1500: loss 2.039313\n",
      "iteration 1000 / 1500: loss 2.074232\n",
      "iteration 1100 / 1500: loss 2.074431\n",
      "iteration 1200 / 1500: loss 2.154705\n",
      "iteration 1300 / 1500: loss 2.071839\n",
      "iteration 1400 / 1500: loss 2.138820\n",
      "iteration 0 / 1500: loss 483.963777\n",
      "iteration 100 / 1500: loss 42.018417\n",
      "iteration 200 / 1500: loss 5.344188\n",
      "iteration 300 / 1500: loss 2.345948\n",
      "iteration 400 / 1500: loss 2.028214\n",
      "iteration 500 / 1500: loss 2.051091\n",
      "iteration 600 / 1500: loss 2.040079\n",
      "iteration 700 / 1500: loss 2.028449\n",
      "iteration 800 / 1500: loss 2.044388\n",
      "iteration 900 / 1500: loss 2.084314\n",
      "iteration 1000 / 1500: loss 1.996559\n",
      "iteration 1100 / 1500: loss 2.048460\n",
      "iteration 1200 / 1500: loss 2.127218\n",
      "iteration 1300 / 1500: loss 2.065385\n",
      "iteration 1400 / 1500: loss 2.036236\n",
      "iteration 0 / 1500: loss 129.724859\n",
      "iteration 100 / 1500: loss 68.163044\n",
      "iteration 200 / 1500: loss 37.115314\n",
      "iteration 300 / 1500: loss 20.711131\n",
      "iteration 400 / 1500: loss 11.875752\n",
      "iteration 500 / 1500: loss 7.264336\n",
      "iteration 600 / 1500: loss 4.720491\n",
      "iteration 700 / 1500: loss 3.461584\n",
      "iteration 800 / 1500: loss 2.772612\n",
      "iteration 900 / 1500: loss 2.358930\n",
      "iteration 1000 / 1500: loss 2.151020\n",
      "iteration 1100 / 1500: loss 2.039967\n",
      "iteration 1200 / 1500: loss 1.966677\n",
      "iteration 1300 / 1500: loss 1.986227\n",
      "iteration 1400 / 1500: loss 1.936799\n",
      "iteration 0 / 1500: loss 119.048511\n",
      "iteration 100 / 1500: loss 65.881316\n",
      "iteration 200 / 1500: loss 37.462084\n",
      "iteration 300 / 1500: loss 21.701796\n",
      "iteration 400 / 1500: loss 12.796772\n",
      "iteration 500 / 1500: loss 8.165580\n",
      "iteration 600 / 1500: loss 5.193225\n",
      "iteration 700 / 1500: loss 3.881102\n",
      "iteration 800 / 1500: loss 2.957745\n",
      "iteration 900 / 1500: loss 2.563984\n",
      "iteration 1000 / 1500: loss 2.274216\n",
      "iteration 1100 / 1500: loss 2.082351\n",
      "iteration 1200 / 1500: loss 1.972512\n",
      "iteration 1300 / 1500: loss 2.052862\n",
      "iteration 1400 / 1500: loss 1.922540\n",
      "iteration 0 / 1500: loss 13.915053\n",
      "iteration 100 / 1500: loss 11.655415\n",
      "iteration 200 / 1500: loss 11.194626\n",
      "iteration 300 / 1500: loss 10.473356\n",
      "iteration 400 / 1500: loss 10.078098\n",
      "iteration 500 / 1500: loss 9.613223\n",
      "iteration 600 / 1500: loss 9.477657\n",
      "iteration 700 / 1500: loss 9.238266\n",
      "iteration 800 / 1500: loss 8.981805\n",
      "iteration 900 / 1500: loss 8.676986\n",
      "iteration 1000 / 1500: loss 8.254767\n",
      "iteration 1100 / 1500: loss 8.081574\n",
      "iteration 1200 / 1500: loss 8.069965\n",
      "iteration 1300 / 1500: loss 7.659567\n",
      "iteration 1400 / 1500: loss 7.436304\n",
      "iteration 0 / 1500: loss 11.528657\n",
      "iteration 100 / 1500: loss 8.051909\n",
      "iteration 200 / 1500: loss 7.616302\n",
      "iteration 300 / 1500: loss 7.401669\n",
      "iteration 400 / 1500: loss 7.191398\n",
      "iteration 500 / 1500: loss 6.862870\n",
      "iteration 600 / 1500: loss 6.837143\n",
      "iteration 700 / 1500: loss 6.559422\n",
      "iteration 800 / 1500: loss 6.482264\n",
      "iteration 900 / 1500: loss 6.285509\n",
      "iteration 1000 / 1500: loss 6.470214\n",
      "iteration 1100 / 1500: loss 6.061826\n",
      "iteration 1200 / 1500: loss 5.962518\n",
      "iteration 1300 / 1500: loss 5.888481\n",
      "iteration 1400 / 1500: loss 5.790806\n",
      "iteration 0 / 1500: loss 101.113115\n",
      "iteration 100 / 1500: loss 68.943997\n",
      "iteration 200 / 1500: loss 48.389215\n",
      "iteration 300 / 1500: loss 34.106844\n",
      "iteration 400 / 1500: loss 24.437217\n",
      "iteration 500 / 1500: loss 17.567555\n",
      "iteration 600 / 1500: loss 12.823230\n",
      "iteration 700 / 1500: loss 9.363469\n",
      "iteration 800 / 1500: loss 7.149205\n",
      "iteration 900 / 1500: loss 5.548649\n",
      "iteration 1000 / 1500: loss 4.511499\n",
      "iteration 1100 / 1500: loss 3.691883\n",
      "iteration 1200 / 1500: loss 3.134642\n",
      "iteration 1300 / 1500: loss 2.744901\n",
      "iteration 1400 / 1500: loss 2.537322\n",
      "iteration 0 / 1500: loss 8935.433738\n",
      "iteration 100 / 1500: loss 2.257909\n",
      "iteration 200 / 1500: loss 2.241611\n",
      "iteration 300 / 1500: loss 2.250578\n",
      "iteration 400 / 1500: loss 2.259726\n",
      "iteration 500 / 1500: loss 2.236790\n",
      "iteration 600 / 1500: loss 2.242596\n",
      "iteration 700 / 1500: loss 2.243806\n",
      "iteration 800 / 1500: loss 2.260083\n",
      "iteration 900 / 1500: loss 2.251706\n",
      "iteration 1000 / 1500: loss 2.242158\n",
      "iteration 1100 / 1500: loss 2.224127\n",
      "iteration 1200 / 1500: loss 2.256190\n",
      "iteration 1300 / 1500: loss 2.253907\n",
      "iteration 1400 / 1500: loss 2.258969\n",
      "iteration 0 / 1500: loss 1482.268086\n",
      "iteration 100 / 1500: loss 7.136062\n",
      "iteration 200 / 1500: loss 2.095036\n",
      "iteration 300 / 1500: loss 2.165401\n",
      "iteration 400 / 1500: loss 2.158493\n",
      "iteration 500 / 1500: loss 2.099675\n",
      "iteration 600 / 1500: loss 2.174518\n",
      "iteration 700 / 1500: loss 2.132916\n",
      "iteration 800 / 1500: loss 2.207536\n",
      "iteration 900 / 1500: loss 2.173625\n",
      "iteration 1000 / 1500: loss 2.157867\n",
      "iteration 1100 / 1500: loss 2.158955\n",
      "iteration 1200 / 1500: loss 2.170124\n",
      "iteration 1300 / 1500: loss 2.167181\n",
      "iteration 1400 / 1500: loss 2.120509\n",
      "iteration 0 / 1500: loss 13062.836475\n",
      "iteration 100 / 1500: loss 2.261595\n",
      "iteration 200 / 1500: loss 2.260036\n",
      "iteration 300 / 1500: loss 2.256133\n",
      "iteration 400 / 1500: loss 2.246089\n",
      "iteration 500 / 1500: loss 2.276919\n",
      "iteration 600 / 1500: loss 2.256358\n",
      "iteration 700 / 1500: loss 2.266037\n",
      "iteration 800 / 1500: loss 2.278816\n",
      "iteration 900 / 1500: loss 2.258737\n",
      "iteration 1000 / 1500: loss 2.263665\n",
      "iteration 1100 / 1500: loss 2.267175\n",
      "iteration 1200 / 1500: loss 2.285293\n",
      "iteration 1300 / 1500: loss 2.271094\n",
      "iteration 1400 / 1500: loss 2.276798\n",
      "iteration 0 / 1500: loss 820.510641\n",
      "iteration 100 / 1500: loss 39.766603\n",
      "iteration 200 / 1500: loss 3.854832\n",
      "iteration 300 / 1500: loss 2.203574\n",
      "iteration 400 / 1500: loss 2.126640\n",
      "iteration 500 / 1500: loss 2.012921\n",
      "iteration 600 / 1500: loss 2.067465\n",
      "iteration 700 / 1500: loss 2.119212\n",
      "iteration 800 / 1500: loss 2.063451\n",
      "iteration 900 / 1500: loss 2.076640\n",
      "iteration 1000 / 1500: loss 2.070300\n",
      "iteration 1100 / 1500: loss 2.138937\n",
      "iteration 1200 / 1500: loss 2.024378\n",
      "iteration 1300 / 1500: loss 2.061120\n",
      "iteration 1400 / 1500: loss 2.061261\n",
      "iteration 0 / 1500: loss 487.935013\n",
      "iteration 100 / 1500: loss 80.532355\n",
      "iteration 200 / 1500: loss 14.779872\n",
      "iteration 300 / 1500: loss 4.091804\n",
      "iteration 400 / 1500: loss 2.420249\n",
      "iteration 500 / 1500: loss 2.059842\n",
      "iteration 600 / 1500: loss 1.978945\n",
      "iteration 700 / 1500: loss 2.033295\n",
      "iteration 800 / 1500: loss 2.025255\n",
      "iteration 900 / 1500: loss 2.067221\n",
      "iteration 1000 / 1500: loss 1.970026\n",
      "iteration 1100 / 1500: loss 2.030202\n",
      "iteration 1200 / 1500: loss 2.066791\n",
      "iteration 1300 / 1500: loss 2.085514\n",
      "iteration 1400 / 1500: loss 2.028556\n",
      "iteration 0 / 1500: loss 126.708913\n",
      "iteration 100 / 1500: loss 78.878420\n",
      "iteration 200 / 1500: loss 50.065166\n",
      "iteration 300 / 1500: loss 32.206631\n",
      "iteration 400 / 1500: loss 21.009513\n",
      "iteration 500 / 1500: loss 13.933001\n",
      "iteration 600 / 1500: loss 9.473971\n",
      "iteration 700 / 1500: loss 6.704359\n",
      "iteration 800 / 1500: loss 4.860454\n",
      "iteration 900 / 1500: loss 3.785628\n",
      "iteration 1000 / 1500: loss 3.116814\n",
      "iteration 1100 / 1500: loss 2.664251\n",
      "iteration 1200 / 1500: loss 2.388185\n",
      "iteration 1300 / 1500: loss 2.179568\n",
      "iteration 1400 / 1500: loss 2.085472\n",
      "iteration 0 / 1500: loss 118.836692\n",
      "iteration 100 / 1500: loss 76.123970\n",
      "iteration 200 / 1500: loss 50.101350\n",
      "iteration 300 / 1500: loss 33.336996\n",
      "iteration 400 / 1500: loss 22.343713\n",
      "iteration 500 / 1500: loss 15.246068\n",
      "iteration 600 / 1500: loss 10.559032\n",
      "iteration 700 / 1500: loss 7.512710\n",
      "iteration 800 / 1500: loss 5.597407\n",
      "iteration 900 / 1500: loss 4.263081\n",
      "iteration 1000 / 1500: loss 3.580588\n",
      "iteration 1100 / 1500: loss 3.005970\n",
      "iteration 1200 / 1500: loss 2.534666\n",
      "iteration 1300 / 1500: loss 2.321890\n",
      "iteration 1400 / 1500: loss 2.272531\n",
      "iteration 0 / 1500: loss 13.592069\n",
      "iteration 100 / 1500: loss 11.050688\n",
      "iteration 200 / 1500: loss 10.265169\n",
      "iteration 300 / 1500: loss 9.425870\n",
      "iteration 400 / 1500: loss 8.735070\n",
      "iteration 500 / 1500: loss 8.246951\n",
      "iteration 600 / 1500: loss 7.765953\n",
      "iteration 700 / 1500: loss 7.385947\n",
      "iteration 800 / 1500: loss 6.781501\n",
      "iteration 900 / 1500: loss 6.529229\n",
      "iteration 1000 / 1500: loss 6.138466\n",
      "iteration 1100 / 1500: loss 5.785079\n",
      "iteration 1200 / 1500: loss 5.565264\n",
      "iteration 1300 / 1500: loss 5.347033\n",
      "iteration 1400 / 1500: loss 5.031007\n",
      "iteration 0 / 1500: loss 10.878566\n",
      "iteration 100 / 1500: loss 7.646517\n",
      "iteration 200 / 1500: loss 6.991386\n",
      "iteration 300 / 1500: loss 6.766372\n",
      "iteration 400 / 1500: loss 6.370829\n",
      "iteration 500 / 1500: loss 6.186090\n",
      "iteration 600 / 1500: loss 5.916102\n",
      "iteration 700 / 1500: loss 5.625417\n",
      "iteration 800 / 1500: loss 5.526710\n",
      "iteration 900 / 1500: loss 5.388241\n",
      "iteration 1000 / 1500: loss 5.065783\n",
      "iteration 1100 / 1500: loss 4.949107\n",
      "iteration 1200 / 1500: loss 4.863337\n",
      "iteration 1300 / 1500: loss 4.664056\n",
      "iteration 1400 / 1500: loss 4.718178\n",
      "iteration 0 / 1500: loss 101.585422\n",
      "iteration 100 / 1500: loss 48.008256\n",
      "iteration 200 / 1500: loss 23.892160\n",
      "iteration 300 / 1500: loss 12.219100\n",
      "iteration 400 / 1500: loss 6.818267\n",
      "iteration 500 / 1500: loss 4.322191\n",
      "iteration 600 / 1500: loss 3.037986\n",
      "iteration 700 / 1500: loss 2.520292\n",
      "iteration 800 / 1500: loss 2.075306\n",
      "iteration 900 / 1500: loss 1.915211\n",
      "iteration 1000 / 1500: loss 1.969690\n",
      "iteration 1100 / 1500: loss 1.984698\n",
      "iteration 1200 / 1500: loss 1.856664\n",
      "iteration 1300 / 1500: loss 1.949282\n",
      "iteration 1400 / 1500: loss 1.914858\n",
      "iteration 0 / 1500: loss 8991.140042\n",
      "iteration 100 / 1500: loss 2.234863\n",
      "iteration 200 / 1500: loss 2.243838\n",
      "iteration 300 / 1500: loss 2.259356\n",
      "iteration 400 / 1500: loss 2.256724\n",
      "iteration 500 / 1500: loss 2.264467\n",
      "iteration 600 / 1500: loss 2.272133\n",
      "iteration 700 / 1500: loss 2.260016\n",
      "iteration 800 / 1500: loss 2.256874\n",
      "iteration 900 / 1500: loss 2.249357\n",
      "iteration 1000 / 1500: loss 2.256499\n",
      "iteration 1100 / 1500: loss 2.257694\n",
      "iteration 1200 / 1500: loss 2.255791\n",
      "iteration 1300 / 1500: loss 2.240069\n",
      "iteration 1400 / 1500: loss 2.246182\n",
      "iteration 0 / 1500: loss 1510.173469\n",
      "iteration 100 / 1500: loss 2.164451\n",
      "iteration 200 / 1500: loss 2.135321\n",
      "iteration 300 / 1500: loss 2.143418\n",
      "iteration 400 / 1500: loss 2.107628\n",
      "iteration 500 / 1500: loss 2.150593\n",
      "iteration 600 / 1500: loss 2.185578\n",
      "iteration 700 / 1500: loss 2.142978\n",
      "iteration 800 / 1500: loss 2.135256\n",
      "iteration 900 / 1500: loss 2.195905\n",
      "iteration 1000 / 1500: loss 2.144777\n",
      "iteration 1100 / 1500: loss 2.151079\n",
      "iteration 1200 / 1500: loss 2.145074\n",
      "iteration 1300 / 1500: loss 2.168944\n",
      "iteration 1400 / 1500: loss 2.125471\n",
      "iteration 0 / 1500: loss 12738.117821\n",
      "iteration 100 / 1500: loss 2.270212\n",
      "iteration 200 / 1500: loss 2.290017\n",
      "iteration 300 / 1500: loss 2.285290\n",
      "iteration 400 / 1500: loss 2.277593\n",
      "iteration 500 / 1500: loss 2.267661\n",
      "iteration 600 / 1500: loss 2.282265\n",
      "iteration 700 / 1500: loss 2.267669\n",
      "iteration 800 / 1500: loss 2.277241\n",
      "iteration 900 / 1500: loss 2.284087\n",
      "iteration 1000 / 1500: loss 2.278480\n",
      "iteration 1100 / 1500: loss 2.266715\n",
      "iteration 1200 / 1500: loss 2.278169\n",
      "iteration 1300 / 1500: loss 2.252255\n",
      "iteration 1400 / 1500: loss 2.261591\n",
      "iteration 0 / 1500: loss 811.430916\n",
      "iteration 100 / 1500: loss 3.468094\n",
      "iteration 200 / 1500: loss 2.060224\n",
      "iteration 300 / 1500: loss 2.072827\n",
      "iteration 400 / 1500: loss 2.119408\n",
      "iteration 500 / 1500: loss 2.073510\n",
      "iteration 600 / 1500: loss 2.108112\n",
      "iteration 700 / 1500: loss 2.094072\n",
      "iteration 800 / 1500: loss 2.015883\n",
      "iteration 900 / 1500: loss 2.007625\n",
      "iteration 1000 / 1500: loss 2.099802\n",
      "iteration 1100 / 1500: loss 2.124357\n",
      "iteration 1200 / 1500: loss 2.096590\n",
      "iteration 1300 / 1500: loss 2.061874\n",
      "iteration 1400 / 1500: loss 2.084551\n",
      "iteration 0 / 1500: loss 487.127426\n",
      "iteration 100 / 1500: loss 13.433214\n",
      "iteration 200 / 1500: loss 2.264560\n",
      "iteration 300 / 1500: loss 2.037835\n",
      "iteration 400 / 1500: loss 2.090571\n",
      "iteration 500 / 1500: loss 2.035140\n",
      "iteration 600 / 1500: loss 2.064494\n",
      "iteration 700 / 1500: loss 2.076495\n",
      "iteration 800 / 1500: loss 2.035605\n",
      "iteration 900 / 1500: loss 2.173939\n",
      "iteration 1000 / 1500: loss 2.089681\n",
      "iteration 1100 / 1500: loss 2.088845\n",
      "iteration 1200 / 1500: loss 2.040736\n",
      "iteration 1300 / 1500: loss 2.032804\n",
      "iteration 1400 / 1500: loss 2.030866\n",
      "iteration 0 / 1500: loss 129.548202\n",
      "iteration 100 / 1500: loss 49.737913\n",
      "iteration 200 / 1500: loss 20.430277\n",
      "iteration 300 / 1500: loss 9.087083\n",
      "iteration 400 / 1500: loss 4.684299\n",
      "iteration 500 / 1500: loss 2.974879\n",
      "iteration 600 / 1500: loss 2.403767\n",
      "iteration 700 / 1500: loss 2.076706\n",
      "iteration 800 / 1500: loss 1.968557\n",
      "iteration 900 / 1500: loss 1.931523\n",
      "iteration 1000 / 1500: loss 1.920541\n",
      "iteration 1100 / 1500: loss 1.942086\n",
      "iteration 1200 / 1500: loss 1.898754\n",
      "iteration 1300 / 1500: loss 2.012565\n",
      "iteration 1400 / 1500: loss 1.982440\n",
      "iteration 0 / 1500: loss 118.582156\n",
      "iteration 100 / 1500: loss 48.761178\n",
      "iteration 200 / 1500: loss 21.322820\n",
      "iteration 300 / 1500: loss 10.019493\n",
      "iteration 400 / 1500: loss 5.380209\n",
      "iteration 500 / 1500: loss 3.355074\n",
      "iteration 600 / 1500: loss 2.577808\n",
      "iteration 700 / 1500: loss 2.182311\n",
      "iteration 800 / 1500: loss 2.022984\n",
      "iteration 900 / 1500: loss 1.935460\n",
      "iteration 1000 / 1500: loss 1.949214\n",
      "iteration 1100 / 1500: loss 1.996387\n",
      "iteration 1200 / 1500: loss 2.053795\n",
      "iteration 1300 / 1500: loss 1.943106\n",
      "iteration 1400 / 1500: loss 1.888469\n",
      "iteration 0 / 1500: loss 14.331495\n",
      "iteration 100 / 1500: loss 8.186928\n",
      "iteration 200 / 1500: loss 6.023182\n",
      "iteration 300 / 1500: loss 4.805674\n",
      "iteration 400 / 1500: loss 3.939268\n",
      "iteration 500 / 1500: loss 3.188032\n",
      "iteration 600 / 1500: loss 2.965504\n",
      "iteration 700 / 1500: loss 2.652077\n",
      "iteration 800 / 1500: loss 2.190386\n",
      "iteration 900 / 1500: loss 2.047532\n",
      "iteration 1000 / 1500: loss 2.072234\n",
      "iteration 1100 / 1500: loss 1.885715\n",
      "iteration 1200 / 1500: loss 1.896132\n",
      "iteration 1300 / 1500: loss 1.784705\n",
      "iteration 1400 / 1500: loss 1.866022\n",
      "iteration 0 / 1500: loss 10.257337\n",
      "iteration 100 / 1500: loss 6.196461\n",
      "iteration 200 / 1500: loss 5.196311\n",
      "iteration 300 / 1500: loss 4.420835\n",
      "iteration 400 / 1500: loss 3.948111\n",
      "iteration 500 / 1500: loss 3.465297\n",
      "iteration 600 / 1500: loss 3.189554\n",
      "iteration 700 / 1500: loss 2.762672\n",
      "iteration 800 / 1500: loss 2.547576\n",
      "iteration 900 / 1500: loss 2.455430\n",
      "iteration 1000 / 1500: loss 2.373885\n",
      "iteration 1100 / 1500: loss 2.264034\n",
      "iteration 1200 / 1500: loss 2.199668\n",
      "iteration 1300 / 1500: loss 2.186724\n",
      "iteration 1400 / 1500: loss 2.018147\n",
      "iteration 0 / 1500: loss 101.092433\n",
      "iteration 100 / 1500: loss 3.833017\n",
      "iteration 200 / 1500: loss 1.979051\n",
      "iteration 300 / 1500: loss 1.958023\n",
      "iteration 400 / 1500: loss 1.895491\n",
      "iteration 500 / 1500: loss 2.024760\n",
      "iteration 600 / 1500: loss 1.977580\n",
      "iteration 700 / 1500: loss 1.976718\n",
      "iteration 800 / 1500: loss 1.958305\n",
      "iteration 900 / 1500: loss 1.951371\n",
      "iteration 1000 / 1500: loss 1.887269\n",
      "iteration 1100 / 1500: loss 1.974786\n",
      "iteration 1200 / 1500: loss 1.951659\n",
      "iteration 1300 / 1500: loss 1.916195\n",
      "iteration 1400 / 1500: loss 1.983306\n",
      "iteration 0 / 1500: loss 8902.539805\n",
      "iteration 100 / 1500: loss 97.921972\n",
      "iteration 200 / 1500: loss 104.074319\n",
      "iteration 300 / 1500: loss 106.025216\n",
      "iteration 400 / 1500: loss 98.832990\n",
      "iteration 500 / 1500: loss 106.096828\n",
      "iteration 600 / 1500: loss 102.503740\n",
      "iteration 700 / 1500: loss 102.961395\n",
      "iteration 800 / 1500: loss 102.129379\n",
      "iteration 900 / 1500: loss 107.597601\n",
      "iteration 1000 / 1500: loss 102.765286\n",
      "iteration 1100 / 1500: loss 100.278556\n",
      "iteration 1200 / 1500: loss 101.435481\n",
      "iteration 1300 / 1500: loss 106.029244\n",
      "iteration 1400 / 1500: loss 109.281561\n",
      "iteration 0 / 1500: loss 1493.683361\n",
      "iteration 100 / 1500: loss 2.242645\n",
      "iteration 200 / 1500: loss 2.201664\n",
      "iteration 300 / 1500: loss 2.216173\n",
      "iteration 400 / 1500: loss 2.146567\n",
      "iteration 500 / 1500: loss 2.196526\n",
      "iteration 600 / 1500: loss 2.238891\n",
      "iteration 700 / 1500: loss 2.153073\n",
      "iteration 800 / 1500: loss 2.247417\n",
      "iteration 900 / 1500: loss 2.165418\n",
      "iteration 1000 / 1500: loss 2.179388\n",
      "iteration 1100 / 1500: loss 2.158859\n",
      "iteration 1200 / 1500: loss 2.252168\n",
      "iteration 1300 / 1500: loss 2.197264\n",
      "iteration 1400 / 1500: loss 2.189521\n",
      "iteration 0 / 1500: loss 13136.534345\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 821.844674\n",
      "iteration 100 / 1500: loss 2.210201\n",
      "iteration 200 / 1500: loss 2.158466\n",
      "iteration 300 / 1500: loss 2.124434\n",
      "iteration 400 / 1500: loss 2.183699\n",
      "iteration 500 / 1500: loss 2.130007\n",
      "iteration 600 / 1500: loss 2.140411\n",
      "iteration 700 / 1500: loss 2.158671\n",
      "iteration 800 / 1500: loss 2.124477\n",
      "iteration 900 / 1500: loss 2.118053\n",
      "iteration 1000 / 1500: loss 2.142294\n",
      "iteration 1100 / 1500: loss 2.119841\n",
      "iteration 1200 / 1500: loss 2.183701\n",
      "iteration 1300 / 1500: loss 2.130024\n",
      "iteration 1400 / 1500: loss 2.117169\n",
      "iteration 0 / 1500: loss 491.241287\n",
      "iteration 100 / 1500: loss 2.106601\n",
      "iteration 200 / 1500: loss 2.078653\n",
      "iteration 300 / 1500: loss 2.050730\n",
      "iteration 400 / 1500: loss 2.091769\n",
      "iteration 500 / 1500: loss 2.081007\n",
      "iteration 600 / 1500: loss 2.148351\n",
      "iteration 700 / 1500: loss 2.111074\n",
      "iteration 800 / 1500: loss 2.109013\n",
      "iteration 900 / 1500: loss 2.125473\n",
      "iteration 1000 / 1500: loss 2.112791\n",
      "iteration 1100 / 1500: loss 2.143143\n",
      "iteration 1200 / 1500: loss 2.093452\n",
      "iteration 1300 / 1500: loss 2.129969\n",
      "iteration 1400 / 1500: loss 2.149575\n",
      "iteration 0 / 1500: loss 127.288827\n",
      "iteration 100 / 1500: loss 2.763345\n",
      "iteration 200 / 1500: loss 1.978818\n",
      "iteration 300 / 1500: loss 1.946114\n",
      "iteration 400 / 1500: loss 1.991636\n",
      "iteration 500 / 1500: loss 1.959002\n",
      "iteration 600 / 1500: loss 1.978984\n",
      "iteration 700 / 1500: loss 2.049065\n",
      "iteration 800 / 1500: loss 1.994833\n",
      "iteration 900 / 1500: loss 1.932349\n",
      "iteration 1000 / 1500: loss 1.948020\n",
      "iteration 1100 / 1500: loss 1.971319\n",
      "iteration 1200 / 1500: loss 1.951500\n",
      "iteration 1300 / 1500: loss 1.954113\n",
      "iteration 1400 / 1500: loss 1.992812\n",
      "iteration 0 / 1500: loss 120.076184\n",
      "iteration 100 / 1500: loss 3.006669\n",
      "iteration 200 / 1500: loss 1.958433\n",
      "iteration 300 / 1500: loss 1.992865\n",
      "iteration 400 / 1500: loss 1.917750\n",
      "iteration 500 / 1500: loss 2.069044\n",
      "iteration 600 / 1500: loss 1.955438\n",
      "iteration 700 / 1500: loss 2.114314\n",
      "iteration 800 / 1500: loss 1.903472\n",
      "iteration 900 / 1500: loss 2.018142\n",
      "iteration 1000 / 1500: loss 1.877574\n",
      "iteration 1100 / 1500: loss 2.046204\n",
      "iteration 1200 / 1500: loss 2.032818\n",
      "iteration 1300 / 1500: loss 2.063703\n",
      "iteration 1400 / 1500: loss 1.992177\n",
      "iteration 0 / 1500: loss 14.787901\n",
      "iteration 100 / 1500: loss 8.793672\n",
      "iteration 200 / 1500: loss 6.970246\n",
      "iteration 300 / 1500: loss 5.457489\n",
      "iteration 400 / 1500: loss 4.444948\n",
      "iteration 500 / 1500: loss 3.785688\n",
      "iteration 600 / 1500: loss 3.224614\n",
      "iteration 700 / 1500: loss 2.874756\n",
      "iteration 800 / 1500: loss 2.608420\n",
      "iteration 900 / 1500: loss 2.281998\n",
      "iteration 1000 / 1500: loss 2.183726\n",
      "iteration 1100 / 1500: loss 2.177537\n",
      "iteration 1200 / 1500: loss 2.111865\n",
      "iteration 1300 / 1500: loss 1.901911\n",
      "iteration 1400 / 1500: loss 1.786787\n",
      "iteration 0 / 1500: loss 10.188542\n",
      "iteration 100 / 1500: loss 6.262725\n",
      "iteration 200 / 1500: loss 5.469087\n",
      "iteration 300 / 1500: loss 4.814312\n",
      "iteration 400 / 1500: loss 4.342917\n",
      "iteration 500 / 1500: loss 3.667937\n",
      "iteration 600 / 1500: loss 3.383878\n",
      "iteration 700 / 1500: loss 3.201258\n",
      "iteration 800 / 1500: loss 2.998808\n",
      "iteration 900 / 1500: loss 2.885404\n",
      "iteration 1000 / 1500: loss 2.607401\n",
      "iteration 1100 / 1500: loss 2.435109\n",
      "iteration 1200 / 1500: loss 2.392309\n",
      "iteration 1300 / 1500: loss 2.183229\n",
      "iteration 1400 / 1500: loss 2.125710\n",
      "iteration 0 / 1500: loss 100.235878\n",
      "iteration 100 / 1500: loss 5.546273\n",
      "iteration 200 / 1500: loss 2.176998\n",
      "iteration 300 / 1500: loss 1.988226\n",
      "iteration 400 / 1500: loss 2.004393\n",
      "iteration 500 / 1500: loss 1.950263\n",
      "iteration 600 / 1500: loss 2.002190\n",
      "iteration 700 / 1500: loss 2.000036\n",
      "iteration 800 / 1500: loss 1.974553\n",
      "iteration 900 / 1500: loss 1.867971\n",
      "iteration 1000 / 1500: loss 1.924660\n",
      "iteration 1100 / 1500: loss 2.004709\n",
      "iteration 1200 / 1500: loss 1.920159\n",
      "iteration 1300 / 1500: loss 1.937433\n",
      "iteration 1400 / 1500: loss 1.977851\n",
      "iteration 0 / 1500: loss 8889.330716\n",
      "iteration 100 / 1500: loss 12.471198\n",
      "iteration 200 / 1500: loss 15.175018\n",
      "iteration 300 / 1500: loss 13.279113\n",
      "iteration 400 / 1500: loss 15.735755\n",
      "iteration 500 / 1500: loss 12.210343\n",
      "iteration 600 / 1500: loss 13.912580\n",
      "iteration 700 / 1500: loss 13.111173\n",
      "iteration 800 / 1500: loss 15.330315\n",
      "iteration 900 / 1500: loss 13.929618\n",
      "iteration 1000 / 1500: loss 12.510841\n",
      "iteration 1100 / 1500: loss 13.152713\n",
      "iteration 1200 / 1500: loss 13.532864\n",
      "iteration 1300 / 1500: loss 13.186695\n",
      "iteration 1400 / 1500: loss 14.357746\n",
      "iteration 0 / 1500: loss 1512.637418\n",
      "iteration 100 / 1500: loss 2.229776\n",
      "iteration 200 / 1500: loss 2.197594\n",
      "iteration 300 / 1500: loss 2.220698\n",
      "iteration 400 / 1500: loss 2.178844\n",
      "iteration 500 / 1500: loss 2.178215\n",
      "iteration 600 / 1500: loss 2.204275\n",
      "iteration 700 / 1500: loss 2.242621\n",
      "iteration 800 / 1500: loss 2.201884\n",
      "iteration 900 / 1500: loss 2.182638\n",
      "iteration 1000 / 1500: loss 2.148891\n",
      "iteration 1100 / 1500: loss 2.147449\n",
      "iteration 1200 / 1500: loss 2.207698\n",
      "iteration 1300 / 1500: loss 2.182248\n",
      "iteration 1400 / 1500: loss 2.192775\n",
      "iteration 0 / 1500: loss 12937.128304\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 826.444982\n",
      "iteration 100 / 1500: loss 2.092896\n",
      "iteration 200 / 1500: loss 2.119238\n",
      "iteration 300 / 1500: loss 2.125823\n",
      "iteration 400 / 1500: loss 2.137967\n",
      "iteration 500 / 1500: loss 2.195587\n",
      "iteration 600 / 1500: loss 2.181517\n",
      "iteration 700 / 1500: loss 2.065404\n",
      "iteration 800 / 1500: loss 2.230347\n",
      "iteration 900 / 1500: loss 2.055104\n",
      "iteration 1000 / 1500: loss 2.133671\n",
      "iteration 1100 / 1500: loss 2.100414\n",
      "iteration 1200 / 1500: loss 2.121170\n",
      "iteration 1300 / 1500: loss 2.184246\n",
      "iteration 1400 / 1500: loss 2.125568\n",
      "iteration 0 / 1500: loss 491.926462\n",
      "iteration 100 / 1500: loss 2.089449\n",
      "iteration 200 / 1500: loss 2.063343\n",
      "iteration 300 / 1500: loss 2.200982\n",
      "iteration 400 / 1500: loss 2.126130\n",
      "iteration 500 / 1500: loss 2.113068\n",
      "iteration 600 / 1500: loss 2.129496\n",
      "iteration 700 / 1500: loss 2.109437\n",
      "iteration 800 / 1500: loss 2.077602\n",
      "iteration 900 / 1500: loss 2.033362\n",
      "iteration 1000 / 1500: loss 2.159497\n",
      "iteration 1100 / 1500: loss 2.131108\n",
      "iteration 1200 / 1500: loss 2.090060\n",
      "iteration 1300 / 1500: loss 2.117950\n",
      "iteration 1400 / 1500: loss 2.170052\n",
      "iteration 0 / 1500: loss 128.584411\n",
      "iteration 100 / 1500: loss 3.874510\n",
      "iteration 200 / 1500: loss 1.953599\n",
      "iteration 300 / 1500: loss 1.898263\n",
      "iteration 400 / 1500: loss 1.891504\n",
      "iteration 500 / 1500: loss 1.922658\n",
      "iteration 600 / 1500: loss 1.986639\n",
      "iteration 700 / 1500: loss 1.945907\n",
      "iteration 800 / 1500: loss 1.989765\n",
      "iteration 900 / 1500: loss 2.048289\n",
      "iteration 1000 / 1500: loss 2.030128\n",
      "iteration 1100 / 1500: loss 1.937187\n",
      "iteration 1200 / 1500: loss 2.011678\n",
      "iteration 1300 / 1500: loss 1.941926\n",
      "iteration 1400 / 1500: loss 1.983499\n",
      "iteration 0 / 1500: loss 119.094132\n",
      "iteration 100 / 1500: loss 4.284736\n",
      "iteration 200 / 1500: loss 1.919144\n",
      "iteration 300 / 1500: loss 2.099053\n",
      "iteration 400 / 1500: loss 1.977833\n",
      "iteration 500 / 1500: loss 1.972295\n",
      "iteration 600 / 1500: loss 2.065290\n",
      "iteration 700 / 1500: loss 1.911017\n",
      "iteration 800 / 1500: loss 1.911911\n",
      "iteration 900 / 1500: loss 1.937788\n",
      "iteration 1000 / 1500: loss 1.971093\n",
      "iteration 1100 / 1500: loss 1.929149\n",
      "iteration 1200 / 1500: loss 1.887140\n",
      "iteration 1300 / 1500: loss 1.958007\n",
      "iteration 1400 / 1500: loss 1.934887\n",
      "iteration 0 / 1500: loss 14.582944\n",
      "iteration 100 / 1500: loss 12.350998\n",
      "iteration 200 / 1500: loss 11.788061\n",
      "iteration 300 / 1500: loss 11.385280\n",
      "iteration 400 / 1500: loss 11.100704\n",
      "iteration 500 / 1500: loss 10.674459\n",
      "iteration 600 / 1500: loss 10.380201\n",
      "iteration 700 / 1500: loss 10.396918\n",
      "iteration 800 / 1500: loss 10.362127\n",
      "iteration 900 / 1500: loss 10.127448\n",
      "iteration 1000 / 1500: loss 9.670316\n",
      "iteration 1100 / 1500: loss 9.737049\n",
      "iteration 1200 / 1500: loss 9.399845\n",
      "iteration 1300 / 1500: loss 9.191346\n",
      "iteration 1400 / 1500: loss 9.083670\n",
      "iteration 0 / 1500: loss 10.807346\n",
      "iteration 100 / 1500: loss 8.308436\n",
      "iteration 200 / 1500: loss 8.355683\n",
      "iteration 300 / 1500: loss 7.677363\n",
      "iteration 400 / 1500: loss 7.343477\n",
      "iteration 500 / 1500: loss 7.137590\n",
      "iteration 600 / 1500: loss 7.137315\n",
      "iteration 700 / 1500: loss 6.961814\n",
      "iteration 800 / 1500: loss 6.927135\n",
      "iteration 900 / 1500: loss 6.827429\n",
      "iteration 1000 / 1500: loss 6.804921\n",
      "iteration 1100 / 1500: loss 6.651219\n",
      "iteration 1200 / 1500: loss 6.917546\n",
      "iteration 1300 / 1500: loss 6.396374\n",
      "iteration 1400 / 1500: loss 6.453473\n",
      "iteration 0 / 1500: loss 101.388650\n",
      "iteration 100 / 1500: loss 83.083432\n",
      "iteration 200 / 1500: loss 69.085508\n",
      "iteration 300 / 1500: loss 57.690770\n",
      "iteration 400 / 1500: loss 48.198669\n",
      "iteration 500 / 1500: loss 40.444829\n",
      "iteration 600 / 1500: loss 33.755463\n",
      "iteration 700 / 1500: loss 28.365115\n",
      "iteration 800 / 1500: loss 23.735780\n",
      "iteration 900 / 1500: loss 20.126314\n",
      "iteration 1000 / 1500: loss 17.062606\n",
      "iteration 1100 / 1500: loss 14.569510\n",
      "iteration 1200 / 1500: loss 12.359896\n",
      "iteration 1300 / 1500: loss 10.590586\n",
      "iteration 1400 / 1500: loss 9.036383\n",
      "iteration 0 / 1500: loss 8922.326257\n",
      "iteration 100 / 1500: loss 2.237945\n",
      "iteration 200 / 1500: loss 2.261836\n",
      "iteration 300 / 1500: loss 2.244204\n",
      "iteration 400 / 1500: loss 2.257807\n",
      "iteration 500 / 1500: loss 2.246435\n",
      "iteration 600 / 1500: loss 2.246665\n",
      "iteration 700 / 1500: loss 2.252591\n",
      "iteration 800 / 1500: loss 2.255993\n",
      "iteration 900 / 1500: loss 2.250354\n",
      "iteration 1000 / 1500: loss 2.262191\n",
      "iteration 1100 / 1500: loss 2.259019\n",
      "iteration 1200 / 1500: loss 2.226538\n",
      "iteration 1300 / 1500: loss 2.246972\n",
      "iteration 1400 / 1500: loss 2.261997\n",
      "iteration 0 / 1500: loss 1509.918139\n",
      "iteration 100 / 1500: loss 86.043632\n",
      "iteration 200 / 1500: loss 6.829885\n",
      "iteration 300 / 1500: loss 2.387206\n",
      "iteration 400 / 1500: loss 2.147382\n",
      "iteration 500 / 1500: loss 2.124227\n",
      "iteration 600 / 1500: loss 2.176019\n",
      "iteration 700 / 1500: loss 2.120743\n",
      "iteration 800 / 1500: loss 2.126220\n",
      "iteration 900 / 1500: loss 2.149431\n",
      "iteration 1000 / 1500: loss 2.160139\n",
      "iteration 1100 / 1500: loss 2.136214\n",
      "iteration 1200 / 1500: loss 2.095956\n",
      "iteration 1300 / 1500: loss 2.142745\n",
      "iteration 1400 / 1500: loss 2.155432\n",
      "iteration 0 / 1500: loss 12890.460667\n",
      "iteration 100 / 1500: loss 2.259732\n",
      "iteration 200 / 1500: loss 2.247428\n",
      "iteration 300 / 1500: loss 2.252018\n",
      "iteration 400 / 1500: loss 2.275501\n",
      "iteration 500 / 1500: loss 2.261423\n",
      "iteration 600 / 1500: loss 2.266409\n",
      "iteration 700 / 1500: loss 2.262401\n",
      "iteration 800 / 1500: loss 2.271366\n",
      "iteration 900 / 1500: loss 2.246837\n",
      "iteration 1000 / 1500: loss 2.251255\n",
      "iteration 1100 / 1500: loss 2.271610\n",
      "iteration 1200 / 1500: loss 2.262895\n",
      "iteration 1300 / 1500: loss 2.276881\n",
      "iteration 1400 / 1500: loss 2.265378\n",
      "iteration 0 / 1500: loss 823.949114\n",
      "iteration 100 / 1500: loss 173.595401\n",
      "iteration 200 / 1500: loss 37.902829\n",
      "iteration 300 / 1500: loss 9.582470\n",
      "iteration 400 / 1500: loss 3.676427\n",
      "iteration 500 / 1500: loss 2.466020\n",
      "iteration 600 / 1500: loss 2.210748\n",
      "iteration 700 / 1500: loss 2.100489\n",
      "iteration 800 / 1500: loss 2.070613\n",
      "iteration 900 / 1500: loss 2.102636\n",
      "iteration 1000 / 1500: loss 2.059627\n",
      "iteration 1100 / 1500: loss 2.032029\n",
      "iteration 1200 / 1500: loss 2.091890\n",
      "iteration 1300 / 1500: loss 2.071228\n",
      "iteration 1400 / 1500: loss 2.151123\n",
      "iteration 0 / 1500: loss 487.153317\n",
      "iteration 100 / 1500: loss 192.879472\n",
      "iteration 200 / 1500: loss 77.530277\n",
      "iteration 300 / 1500: loss 31.909854\n",
      "iteration 400 / 1500: loss 13.918105\n",
      "iteration 500 / 1500: loss 6.718752\n",
      "iteration 600 / 1500: loss 3.934463\n",
      "iteration 700 / 1500: loss 2.787229\n",
      "iteration 800 / 1500: loss 2.357454\n",
      "iteration 900 / 1500: loss 2.153808\n",
      "iteration 1000 / 1500: loss 2.118771\n",
      "iteration 1100 / 1500: loss 1.984297\n",
      "iteration 1200 / 1500: loss 2.067210\n",
      "iteration 1300 / 1500: loss 2.053960\n",
      "iteration 1400 / 1500: loss 2.018959\n",
      "iteration 0 / 1500: loss 129.957659\n",
      "iteration 100 / 1500: loss 101.654640\n",
      "iteration 200 / 1500: loss 80.198529\n",
      "iteration 300 / 1500: loss 63.422631\n",
      "iteration 400 / 1500: loss 50.480048\n",
      "iteration 500 / 1500: loss 40.228858\n",
      "iteration 600 / 1500: loss 32.115575\n",
      "iteration 700 / 1500: loss 25.858853\n",
      "iteration 800 / 1500: loss 20.800223\n",
      "iteration 900 / 1500: loss 16.827409\n",
      "iteration 1000 / 1500: loss 13.622572\n",
      "iteration 1100 / 1500: loss 11.160300\n",
      "iteration 1200 / 1500: loss 9.189031\n",
      "iteration 1300 / 1500: loss 7.654939\n",
      "iteration 1400 / 1500: loss 6.487717\n",
      "iteration 0 / 1500: loss 119.991156\n",
      "iteration 100 / 1500: loss 95.241530\n",
      "iteration 200 / 1500: loss 76.495609\n",
      "iteration 300 / 1500: loss 61.925728\n",
      "iteration 400 / 1500: loss 49.986016\n",
      "iteration 500 / 1500: loss 40.457809\n",
      "iteration 600 / 1500: loss 32.879349\n",
      "iteration 700 / 1500: loss 26.892477\n",
      "iteration 800 / 1500: loss 21.875742\n",
      "iteration 900 / 1500: loss 18.054614\n",
      "iteration 1000 / 1500: loss 14.882195\n",
      "iteration 1100 / 1500: loss 12.230532\n",
      "iteration 1200 / 1500: loss 10.236377\n",
      "iteration 1300 / 1500: loss 8.545311\n",
      "iteration 1400 / 1500: loss 7.370968\n",
      "iteration 0 / 1500: loss 14.717179\n",
      "iteration 100 / 1500: loss 11.611577\n",
      "iteration 200 / 1500: loss 10.913479\n",
      "iteration 300 / 1500: loss 10.331527\n",
      "iteration 400 / 1500: loss 10.108069\n",
      "iteration 500 / 1500: loss 9.472005\n",
      "iteration 600 / 1500: loss 9.523766\n",
      "iteration 700 / 1500: loss 9.024095\n",
      "iteration 800 / 1500: loss 8.579278\n",
      "iteration 900 / 1500: loss 8.328838\n",
      "iteration 1000 / 1500: loss 8.081479\n",
      "iteration 1100 / 1500: loss 7.795809\n",
      "iteration 1200 / 1500: loss 7.682969\n",
      "iteration 1300 / 1500: loss 7.482484\n",
      "iteration 1400 / 1500: loss 7.102200\n",
      "iteration 0 / 1500: loss 10.343863\n",
      "iteration 100 / 1500: loss 8.084729\n",
      "iteration 200 / 1500: loss 7.461440\n",
      "iteration 300 / 1500: loss 7.205819\n",
      "iteration 400 / 1500: loss 7.089705\n",
      "iteration 500 / 1500: loss 6.662148\n",
      "iteration 600 / 1500: loss 6.574381\n",
      "iteration 700 / 1500: loss 6.150697\n",
      "iteration 800 / 1500: loss 6.199435\n",
      "iteration 900 / 1500: loss 6.122121\n",
      "iteration 1000 / 1500: loss 6.028128\n",
      "iteration 1100 / 1500: loss 5.953794\n",
      "iteration 1200 / 1500: loss 6.017121\n",
      "iteration 1300 / 1500: loss 5.771065\n",
      "iteration 1400 / 1500: loss 5.525879\n",
      "iteration 0 / 1500: loss 100.804137\n",
      "iteration 100 / 1500: loss 66.427753\n",
      "iteration 200 / 1500: loss 45.182309\n",
      "iteration 300 / 1500: loss 30.762260\n",
      "iteration 400 / 1500: loss 21.375886\n",
      "iteration 500 / 1500: loss 15.044781\n",
      "iteration 600 / 1500: loss 10.674139\n",
      "iteration 700 / 1500: loss 7.817363\n",
      "iteration 800 / 1500: loss 5.876908\n",
      "iteration 900 / 1500: loss 4.466253\n",
      "iteration 1000 / 1500: loss 3.656011\n",
      "iteration 1100 / 1500: loss 3.126415\n",
      "iteration 1200 / 1500: loss 2.797646\n",
      "iteration 1300 / 1500: loss 2.468574\n",
      "iteration 1400 / 1500: loss 2.312501\n",
      "iteration 0 / 1500: loss 8917.645296\n",
      "iteration 100 / 1500: loss 2.259887\n",
      "iteration 200 / 1500: loss 2.240263\n",
      "iteration 300 / 1500: loss 2.254961\n",
      "iteration 400 / 1500: loss 2.249026\n",
      "iteration 500 / 1500: loss 2.255555\n",
      "iteration 600 / 1500: loss 2.251790\n",
      "iteration 700 / 1500: loss 2.256371\n",
      "iteration 800 / 1500: loss 2.237333\n",
      "iteration 900 / 1500: loss 2.260439\n",
      "iteration 1000 / 1500: loss 2.230082\n",
      "iteration 1100 / 1500: loss 2.256089\n",
      "iteration 1200 / 1500: loss 2.262536\n",
      "iteration 1300 / 1500: loss 2.254862\n",
      "iteration 1400 / 1500: loss 2.252550\n",
      "iteration 0 / 1500: loss 1505.376188\n",
      "iteration 100 / 1500: loss 5.028048\n",
      "iteration 200 / 1500: loss 2.172199\n",
      "iteration 300 / 1500: loss 2.167946\n",
      "iteration 400 / 1500: loss 2.145843\n",
      "iteration 500 / 1500: loss 2.181096\n",
      "iteration 600 / 1500: loss 2.166913\n",
      "iteration 700 / 1500: loss 2.126451\n",
      "iteration 800 / 1500: loss 2.143655\n",
      "iteration 900 / 1500: loss 2.092992\n",
      "iteration 1000 / 1500: loss 2.199336\n",
      "iteration 1100 / 1500: loss 2.110392\n",
      "iteration 1200 / 1500: loss 2.131095\n",
      "iteration 1300 / 1500: loss 2.161974\n",
      "iteration 1400 / 1500: loss 2.131585\n",
      "iteration 0 / 1500: loss 13029.298113\n",
      "iteration 100 / 1500: loss 2.268319\n",
      "iteration 200 / 1500: loss 2.269129\n",
      "iteration 300 / 1500: loss 2.271035\n",
      "iteration 400 / 1500: loss 2.263911\n",
      "iteration 500 / 1500: loss 2.268392\n",
      "iteration 600 / 1500: loss 2.268010\n",
      "iteration 700 / 1500: loss 2.256957\n",
      "iteration 800 / 1500: loss 2.264905\n",
      "iteration 900 / 1500: loss 2.254879\n",
      "iteration 1000 / 1500: loss 2.280929\n",
      "iteration 1100 / 1500: loss 2.266973\n",
      "iteration 1200 / 1500: loss 2.244373\n",
      "iteration 1300 / 1500: loss 2.278830\n",
      "iteration 1400 / 1500: loss 2.279011\n",
      "iteration 0 / 1500: loss 818.478331\n",
      "iteration 100 / 1500: loss 29.949717\n",
      "iteration 200 / 1500: loss 3.099928\n",
      "iteration 300 / 1500: loss 2.125928\n",
      "iteration 400 / 1500: loss 2.089148\n",
      "iteration 500 / 1500: loss 2.111835\n",
      "iteration 600 / 1500: loss 2.068223\n",
      "iteration 700 / 1500: loss 2.118956\n",
      "iteration 800 / 1500: loss 2.127011\n",
      "iteration 900 / 1500: loss 2.128165\n",
      "iteration 1000 / 1500: loss 2.082525\n",
      "iteration 1100 / 1500: loss 2.092051\n",
      "iteration 1200 / 1500: loss 2.135639\n",
      "iteration 1300 / 1500: loss 2.089667\n",
      "iteration 1400 / 1500: loss 2.076732\n",
      "iteration 0 / 1500: loss 487.582157\n",
      "iteration 100 / 1500: loss 67.752040\n",
      "iteration 200 / 1500: loss 10.913733\n",
      "iteration 300 / 1500: loss 3.207609\n",
      "iteration 400 / 1500: loss 2.225145\n",
      "iteration 500 / 1500: loss 2.092981\n",
      "iteration 600 / 1500: loss 2.073621\n",
      "iteration 700 / 1500: loss 1.985725\n",
      "iteration 800 / 1500: loss 2.039254\n",
      "iteration 900 / 1500: loss 1.998937\n",
      "iteration 1000 / 1500: loss 2.053668\n",
      "iteration 1100 / 1500: loss 2.047278\n",
      "iteration 1200 / 1500: loss 2.019401\n",
      "iteration 1300 / 1500: loss 2.027478\n",
      "iteration 1400 / 1500: loss 2.014446\n",
      "iteration 0 / 1500: loss 130.361785\n",
      "iteration 100 / 1500: loss 77.014614\n",
      "iteration 200 / 1500: loss 46.991217\n",
      "iteration 300 / 1500: loss 28.911912\n",
      "iteration 400 / 1500: loss 18.127711\n",
      "iteration 500 / 1500: loss 11.718690\n",
      "iteration 600 / 1500: loss 7.812111\n",
      "iteration 700 / 1500: loss 5.369654\n",
      "iteration 800 / 1500: loss 4.045261\n",
      "iteration 900 / 1500: loss 3.235727\n",
      "iteration 1000 / 1500: loss 2.723685\n",
      "iteration 1100 / 1500: loss 2.427286\n",
      "iteration 1200 / 1500: loss 2.197334\n",
      "iteration 1300 / 1500: loss 2.150504\n",
      "iteration 1400 / 1500: loss 2.088107\n",
      "iteration 0 / 1500: loss 119.990921\n",
      "iteration 100 / 1500: loss 74.284487\n",
      "iteration 200 / 1500: loss 46.919749\n",
      "iteration 300 / 1500: loss 30.104851\n",
      "iteration 400 / 1500: loss 19.284679\n",
      "iteration 500 / 1500: loss 12.867712\n",
      "iteration 600 / 1500: loss 8.677990\n",
      "iteration 700 / 1500: loss 6.213521\n",
      "iteration 800 / 1500: loss 4.566231\n",
      "iteration 900 / 1500: loss 3.634237\n",
      "iteration 1000 / 1500: loss 2.998243\n",
      "iteration 1100 / 1500: loss 2.620557\n",
      "iteration 1200 / 1500: loss 2.379348\n",
      "iteration 1300 / 1500: loss 2.149704\n",
      "iteration 1400 / 1500: loss 2.011237\n",
      "lr 5.899334e-08 reg 3.140780e+02 train accuracy: 0.235265 val accuracy: 0.219000\n",
      "lr 5.899334e-08 reg 5.717225e+02 train accuracy: 0.222551 val accuracy: 0.255000\n",
      "lr 5.899334e-08 reg 6.242959e+03 train accuracy: 0.249449 val accuracy: 0.256000\n",
      "lr 5.899334e-08 reg 7.386535e+03 train accuracy: 0.262633 val accuracy: 0.277000\n",
      "lr 5.899334e-08 reg 7.980706e+03 train accuracy: 0.264918 val accuracy: 0.275000\n",
      "lr 5.899334e-08 reg 3.141398e+04 train accuracy: 0.334755 val accuracy: 0.344000\n",
      "lr 5.899334e-08 reg 5.303393e+04 train accuracy: 0.324653 val accuracy: 0.339000\n",
      "lr 5.899334e-08 reg 9.758980e+04 train accuracy: 0.303286 val accuracy: 0.322000\n",
      "lr 5.899334e-08 reg 5.819933e+05 train accuracy: 0.274367 val accuracy: 0.287000\n",
      "lr 5.899334e-08 reg 8.410742e+05 train accuracy: 0.259143 val accuracy: 0.272000\n",
      "lr 1.324657e-07 reg 3.140780e+02 train accuracy: 0.259082 val accuracy: 0.263000\n",
      "lr 1.324657e-07 reg 5.717225e+02 train accuracy: 0.272490 val accuracy: 0.285000\n",
      "lr 1.324657e-07 reg 6.242959e+03 train accuracy: 0.331898 val accuracy: 0.323000\n",
      "lr 1.324657e-07 reg 7.386535e+03 train accuracy: 0.340204 val accuracy: 0.348000\n",
      "lr 1.324657e-07 reg 7.980706e+03 train accuracy: 0.346490 val accuracy: 0.348000\n",
      "lr 1.324657e-07 reg 3.141398e+04 train accuracy: 0.345469 val accuracy: 0.363000\n",
      "lr 1.324657e-07 reg 5.303393e+04 train accuracy: 0.323776 val accuracy: 0.340000\n",
      "lr 1.324657e-07 reg 9.758980e+04 train accuracy: 0.302184 val accuracy: 0.322000\n",
      "lr 1.324657e-07 reg 5.819933e+05 train accuracy: 0.251122 val accuracy: 0.267000\n",
      "lr 1.324657e-07 reg 8.410742e+05 train accuracy: 0.253735 val accuracy: 0.273000\n",
      "lr 1.466799e-07 reg 3.140780e+02 train accuracy: 0.272959 val accuracy: 0.269000\n",
      "lr 1.466799e-07 reg 5.717225e+02 train accuracy: 0.277837 val accuracy: 0.293000\n",
      "lr 1.466799e-07 reg 6.242959e+03 train accuracy: 0.345653 val accuracy: 0.352000\n",
      "lr 1.466799e-07 reg 7.386535e+03 train accuracy: 0.351265 val accuracy: 0.354000\n",
      "lr 1.466799e-07 reg 7.980706e+03 train accuracy: 0.354143 val accuracy: 0.353000\n",
      "lr 1.466799e-07 reg 3.141398e+04 train accuracy: 0.344347 val accuracy: 0.358000\n",
      "lr 1.466799e-07 reg 5.303393e+04 train accuracy: 0.326857 val accuracy: 0.345000\n",
      "lr 1.466799e-07 reg 9.758980e+04 train accuracy: 0.306449 val accuracy: 0.322000\n",
      "lr 1.466799e-07 reg 5.819933e+05 train accuracy: 0.262306 val accuracy: 0.276000\n",
      "lr 1.466799e-07 reg 8.410742e+05 train accuracy: 0.246388 val accuracy: 0.240000\n",
      "lr 2.872320e-07 reg 3.140780e+02 train accuracy: 0.301571 val accuracy: 0.289000\n",
      "lr 2.872320e-07 reg 5.717225e+02 train accuracy: 0.309327 val accuracy: 0.310000\n",
      "lr 2.872320e-07 reg 6.242959e+03 train accuracy: 0.380469 val accuracy: 0.389000\n",
      "lr 2.872320e-07 reg 7.386535e+03 train accuracy: 0.377857 val accuracy: 0.384000\n",
      "lr 2.872320e-07 reg 7.980706e+03 train accuracy: 0.378347 val accuracy: 0.390000\n",
      "lr 2.872320e-07 reg 3.141398e+04 train accuracy: 0.344082 val accuracy: 0.350000\n",
      "lr 2.872320e-07 reg 5.303393e+04 train accuracy: 0.329224 val accuracy: 0.345000\n",
      "lr 2.872320e-07 reg 9.758980e+04 train accuracy: 0.310980 val accuracy: 0.327000\n",
      "lr 2.872320e-07 reg 5.819933e+05 train accuracy: 0.261592 val accuracy: 0.267000\n",
      "lr 2.872320e-07 reg 8.410742e+05 train accuracy: 0.252367 val accuracy: 0.261000\n",
      "lr 3.152206e-07 reg 3.140780e+02 train accuracy: 0.309490 val accuracy: 0.311000\n",
      "lr 3.152206e-07 reg 5.717225e+02 train accuracy: 0.315857 val accuracy: 0.314000\n",
      "lr 3.152206e-07 reg 6.242959e+03 train accuracy: 0.383041 val accuracy: 0.389000\n",
      "lr 3.152206e-07 reg 7.386535e+03 train accuracy: 0.378184 val accuracy: 0.388000\n",
      "lr 3.152206e-07 reg 7.980706e+03 train accuracy: 0.380612 val accuracy: 0.392000\n",
      "lr 3.152206e-07 reg 3.141398e+04 train accuracy: 0.339673 val accuracy: 0.361000\n",
      "lr 3.152206e-07 reg 5.303393e+04 train accuracy: 0.327735 val accuracy: 0.351000\n",
      "lr 3.152206e-07 reg 9.758980e+04 train accuracy: 0.316531 val accuracy: 0.326000\n",
      "lr 3.152206e-07 reg 5.819933e+05 train accuracy: 0.249265 val accuracy: 0.263000\n",
      "lr 3.152206e-07 reg 8.410742e+05 train accuracy: 0.259143 val accuracy: 0.278000\n",
      "lr 3.913951e-07 reg 3.140780e+02 train accuracy: 0.315102 val accuracy: 0.324000\n",
      "lr 3.913951e-07 reg 5.717225e+02 train accuracy: 0.332163 val accuracy: 0.340000\n",
      "lr 3.913951e-07 reg 6.242959e+03 train accuracy: 0.384102 val accuracy: 0.398000\n",
      "lr 3.913951e-07 reg 7.386535e+03 train accuracy: 0.381490 val accuracy: 0.403000\n",
      "lr 3.913951e-07 reg 7.980706e+03 train accuracy: 0.374020 val accuracy: 0.380000\n",
      "lr 3.913951e-07 reg 3.141398e+04 train accuracy: 0.341959 val accuracy: 0.354000\n",
      "lr 3.913951e-07 reg 5.303393e+04 train accuracy: 0.319694 val accuracy: 0.326000\n",
      "lr 3.913951e-07 reg 9.758980e+04 train accuracy: 0.300592 val accuracy: 0.313000\n",
      "lr 3.913951e-07 reg 5.819933e+05 train accuracy: 0.246633 val accuracy: 0.250000\n",
      "lr 3.913951e-07 reg 8.410742e+05 train accuracy: 0.235265 val accuracy: 0.253000\n",
      "lr 5.890788e-07 reg 3.140780e+02 train accuracy: 0.343857 val accuracy: 0.333000\n",
      "lr 5.890788e-07 reg 5.717225e+02 train accuracy: 0.357347 val accuracy: 0.351000\n",
      "lr 5.890788e-07 reg 6.242959e+03 train accuracy: 0.384347 val accuracy: 0.397000\n",
      "lr 5.890788e-07 reg 7.386535e+03 train accuracy: 0.378755 val accuracy: 0.396000\n",
      "lr 5.890788e-07 reg 7.980706e+03 train accuracy: 0.375510 val accuracy: 0.403000\n",
      "lr 5.890788e-07 reg 3.141398e+04 train accuracy: 0.338286 val accuracy: 0.351000\n",
      "lr 5.890788e-07 reg 5.303393e+04 train accuracy: 0.328306 val accuracy: 0.330000\n",
      "lr 5.890788e-07 reg 9.758980e+04 train accuracy: 0.298776 val accuracy: 0.311000\n",
      "lr 5.890788e-07 reg 5.819933e+05 train accuracy: 0.243776 val accuracy: 0.257000\n",
      "lr 5.890788e-07 reg 8.410742e+05 train accuracy: 0.245347 val accuracy: 0.268000\n",
      "lr 2.580392e-06 reg 3.140780e+02 train accuracy: 0.399918 val accuracy: 0.404000\n",
      "lr 2.580392e-06 reg 5.717225e+02 train accuracy: 0.397837 val accuracy: 0.394000\n",
      "lr 2.580392e-06 reg 6.242959e+03 train accuracy: 0.361224 val accuracy: 0.369000\n",
      "lr 2.580392e-06 reg 7.386535e+03 train accuracy: 0.363367 val accuracy: 0.359000\n",
      "lr 2.580392e-06 reg 7.980706e+03 train accuracy: 0.358633 val accuracy: 0.352000\n",
      "lr 2.580392e-06 reg 3.141398e+04 train accuracy: 0.321429 val accuracy: 0.330000\n",
      "lr 2.580392e-06 reg 5.303393e+04 train accuracy: 0.270122 val accuracy: 0.268000\n",
      "lr 2.580392e-06 reg 9.758980e+04 train accuracy: 0.245980 val accuracy: 0.246000\n",
      "lr 2.580392e-06 reg 5.819933e+05 train accuracy: 0.088327 val accuracy: 0.102000\n",
      "lr 2.580392e-06 reg 8.410742e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.105587e-06 reg 3.140780e+02 train accuracy: 0.415224 val accuracy: 0.402000\n",
      "lr 3.105587e-06 reg 5.717225e+02 train accuracy: 0.407878 val accuracy: 0.384000\n",
      "lr 3.105587e-06 reg 6.242959e+03 train accuracy: 0.357449 val accuracy: 0.358000\n",
      "lr 3.105587e-06 reg 7.386535e+03 train accuracy: 0.316714 val accuracy: 0.346000\n",
      "lr 3.105587e-06 reg 7.980706e+03 train accuracy: 0.362020 val accuracy: 0.379000\n",
      "lr 3.105587e-06 reg 3.141398e+04 train accuracy: 0.306776 val accuracy: 0.329000\n",
      "lr 3.105587e-06 reg 5.303393e+04 train accuracy: 0.273551 val accuracy: 0.279000\n",
      "lr 3.105587e-06 reg 9.758980e+04 train accuracy: 0.270755 val accuracy: 0.273000\n",
      "lr 3.105587e-06 reg 5.819933e+05 train accuracy: 0.131020 val accuracy: 0.119000\n",
      "lr 3.105587e-06 reg 8.410742e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.740896e-06 reg 3.140780e+02 train accuracy: 0.392878 val accuracy: 0.365000\n",
      "lr 3.740896e-06 reg 5.717225e+02 train accuracy: 0.384939 val accuracy: 0.370000\n",
      "lr 3.740896e-06 reg 6.242959e+03 train accuracy: 0.362184 val accuracy: 0.377000\n",
      "lr 3.740896e-06 reg 7.386535e+03 train accuracy: 0.324347 val accuracy: 0.335000\n",
      "lr 3.740896e-06 reg 7.980706e+03 train accuracy: 0.359959 val accuracy: 0.379000\n",
      "lr 3.740896e-06 reg 3.141398e+04 train accuracy: 0.302122 val accuracy: 0.311000\n",
      "lr 3.740896e-06 reg 5.303393e+04 train accuracy: 0.272245 val accuracy: 0.279000\n",
      "lr 3.740896e-06 reg 9.758980e+04 train accuracy: 0.233020 val accuracy: 0.223000\n",
      "lr 3.740896e-06 reg 5.819933e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.740896e-06 reg 8.410742e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.404000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = 10**(-1*np.random.uniform(5,8,10))\n",
    "regularization_strengths = 10**np.random.uniform(2,6,10)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for ii in xrange(len(learning_rates)):\n",
    "    for jj in xrange(len(regularization_strengths)):\n",
    "        softmax = Softmax()\n",
    "        lr = learning_rates[ii]\n",
    "        reg = regularization_strengths[jj]\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                                  num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
    "        if (val_accuracy > best_val):\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.370000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEEQBCf4v+Cp+NLDHuspAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f438b6b6310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
